{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "## Deep Learning\n",
    "## Project: Build a Traffic Sign Recognition Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utility packages\n",
    "import numpy as np\n",
    "\n",
    "# Printing helper packages\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# Load training and testing data\n",
    "\n",
    "training_file = 'train.p'\n",
    "testing_file = 'test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "### Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 39209\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "### test set to ensure every thing is valid\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "### Data Set Summary\n",
    "\n",
    "# Number of training examples\n",
    "n_train = len(X_train)\n",
    "\n",
    "# Number of testing examples.\n",
    "n_test = len(X_test)\n",
    "\n",
    "# What's the shape of an traffic sign image?\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "# How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0), return_counts=True)[0])\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include an exploratory visualization of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc0903f6940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The work on this cell is based on this\n",
    "#https://github.com/deacons2016/traffic_signs/blob/master/Traffic_Sign_Classifier.ipynb\n",
    "def load_label_file(file):\n",
    "    lines = open(file).readlines()[1:]\n",
    "    lines = [line.strip().split(\",\") for line in lines]\n",
    "    label_descs = dict([ (int(classid), desc) for classid, desc in lines])\n",
    "    return label_descs\n",
    "\n",
    "label_descriptions = load_label_file(\"signnames.csv\")\n",
    "\n",
    "# the histogram of the training data\n",
    "n, bins, patches = plt.hist(y_train, n_classes, facecolor='green')\n",
    "\n",
    "plt.xlabel('Signs')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.title('Number of Occurrences in Training Set')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Road narrows on the right 24\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7A7xf4XWmCqT4fxoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc0900903c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Show random sample of training set\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image)\n",
    "print(label_descriptions.get(y_train[index]),y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "### Pre-process the Data Set (normalization, grayscale, etc.)\n",
    "\n",
    "I did four things for data preprocessing:\n",
    "- Convert RGB to YUV and use Y channel only.\n",
    "- Normalize Y channel\n",
    "- Create augmented images based on translating image and rotating it.\n",
    "- Write Python generator function that create 5 new images for each patch and apply the steps mentioned above.\n",
    "\n",
    "\n",
    "Data preprocessing step is inpired by this work:\n",
    "http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf\n",
    "\n",
    "#### This will increase batch size from 128 to (5 * 128) + 128 = 768\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### add helper functions to randomly translate and rotate images\n",
    "import cv2\n",
    "\n",
    "def translateImage(image):\n",
    "    t_x = (np.random.randn(1)*.5)[0]\n",
    "    t_y = (np.random.randn(1)*.5)[0]\n",
    "    #print(t_x,t_y)\n",
    "    rows,cols,_ = image.shape\n",
    "    M = np.float32([[1,0,t_x],[0,1,t_y]])\n",
    "    dst = cv2.warpAffine(image,M,(cols,rows))\n",
    "    return dst\n",
    "\n",
    "def rotateImage(image):\n",
    "    theta = (np.random.randn(1)*5)[0]\n",
    "    #print(theta)\n",
    "    rows,cols,_ = image.shape\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2),theta,1)\n",
    "    dst = cv2.warpAffine(image,M,(cols,rows))\n",
    "    return dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### add helper function to create 5 new augmented images for each image\n",
    "\n",
    "def generateAugmentedImages(X_data,y_data):\n",
    "    num_images = 5 * len(X_data)\n",
    "    x_aug = np.zeros(shape=(num_images,32,32,3)).astype(np.uint8)\n",
    "    y_aug = np.zeros(shape=(num_images))\n",
    "    aug_idx = 0\n",
    "    for idx, img in enumerate(X_data):\n",
    "        lbl = y_data[idx]\n",
    "        for itr in range(5):\n",
    "            new_image = translateImage(rotateImage(img))\n",
    "            x_aug[aug_idx] = new_image\n",
    "            y_aug[aug_idx] = lbl\n",
    "            aug_idx = aug_idx + 1\n",
    "    X_data = np.concatenate((X_data, x_aug), axis=0)\n",
    "    y_data = np.concatenate((y_data, y_aug), axis=0)\n",
    "    \n",
    "    return X_data, y_data\n",
    "    \n",
    "\n",
    "#x,y = generateAugmentedImages(X_train[0:128],y_train[0:128])\n",
    "\n",
    "#print(len(x),len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### convert to YUV, extract Y and normalize it\n",
    "\n",
    "def imagestoY(batch_x):\n",
    "    for x in batch_x:\n",
    "        img_out = cv2.cvtColor(x, cv2.COLOR_RGB2YUV)\n",
    "        img_out = img_out[:,:,0].reshape(32,32,1)\n",
    "        img_out = img_out - np.mean(img_out)\n",
    "        yield img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Python generator function that applied to fetch batches and do data preprocessing        \n",
    "# https://github.com/justheuristic/prefetch_generator\n",
    "def iterate_minibatches(num_examples, batch_size, X_data, y_data, is_training = 1):\n",
    "    for offset in range(0, num_examples, batch_size):\n",
    "        end = offset + batch_size\n",
    "        batch_x, batch_y = X_data[offset:end], y_data[offset:end]\n",
    "        if is_training == 1:\n",
    "            batch_x, batch_y = generateAugmentedImages(batch_x, batch_y)\n",
    "            batch_x, batch_y = shuffle(batch_x, batch_y)\n",
    "        batch_x = imagestoY(batch_x)\n",
    "        yield batch_x,batch_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Training, Validation and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples before augmentaion = 31367 and after augmentation = 188202\n",
      "Number of validation examples = 7842\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation =   train_test_split(X_train, y_train, test_size=.2, random_state=0)\n",
    "\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_validation) == len(y_validation))\n",
    "\n",
    "# Number of training examples\n",
    "n_train = len(X_train)\n",
    "\n",
    "# Number of validation examples.\n",
    "n_validation = len(X_validation)\n",
    "\n",
    "print(\"Number of training examples before augmentaion = {0} and after augmentation = {1}\".format(n_train, n_train*6) )\n",
    "print(\"Number of validation examples =\", n_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "Model Arch is the same as LeNet but changed the depth of CONV layers to be 12 instead of 6 for the first CONV layer and 48 instead of 12 for the second one.\n",
    "\n",
    "I also added a fully connected layer with 1200-400 followed by 400-120, 120-84, and 84-43\n",
    "\n",
    "Finally, I added a dropout layer for each FC layer with keep prop of 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x12.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 12), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(12))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # Pooling. Input = 28x28x12. Output = 14x14x12.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Layer 2: Convolutional. Output = 10x10x48.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 12, 48), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(48))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # Pooling. Input = 10x10x48. Output = 5x5x48.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Flatten. Input = 5x5x48. Output = 1200.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # Layer 3: Fully Connected. Input = 1200. Output = 400.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(1200, 400), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(400))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    \n",
    "    # Regularization.\n",
    "    fc1    = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(120))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    \n",
    "    # Regularization.\n",
    "    fc2    = tf.nn.dropout(fc2, keep_prob)\n",
    "\n",
    "    # Layer 5: Fully Connected. Input = 120. Output = 84.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(84))\n",
    "    fc3    = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    # Activation.\n",
    "    fc3    = tf.nn.relu(fc3)\n",
    "    \n",
    "    # Regularization.\n",
    "    fc3    = tf.nn.dropout(fc3, keep_prob)\n",
    "    \n",
    "    # Layer 6: Fully Connected. Input = 84. Output = 43.\n",
    "    fc4_W  = tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean = mu, stddev = sigma))\n",
    "    fc4_b  = tf.Variable(tf.zeros(n_classes))\n",
    "    logits = tf.matmul(fc3, fc4_W) + fc4_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, n_classes)\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model\n",
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "from prefetch_generator import BackgroundGenerator\n",
    "from confusionmatrix import ConfusionMatrix\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    confusion_valid = ConfusionMatrix(n_classes)\n",
    "    sess = tf.get_default_session()\n",
    "    for b_x,b_y in BackgroundGenerator(iterate_minibatches(num_examples,BATCH_SIZE, X_data, y_data,0)):\n",
    "        batch_x = np.array([x for x in b_x]) #np.fromiter(b_x,float)\n",
    "        batch_y = np.array([y for y in b_y]) #np.fromiter(b_y,float)\n",
    "        res = sess.run(logits, feed_dict={x: batch_x, y: batch_y, keep_prob: 1})\n",
    "        preds = np.argmax(res, 1)\n",
    "        confusion_valid.batch_add(batch_y, preds)\n",
    "    return confusion_valid #.accuracy()\n",
    "        #accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        #total_accuracy += (accuracy * len(batch_x))\n",
    "    #return total_accuracy / num_examples\n",
    "    #for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        #batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        #accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        #total_accuracy += (accuracy * len(batch_x))\n",
    "    #return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.725\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.945\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.969\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.978\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.987\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.990\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.992\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.992\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.994\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.994\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for b_x,b_y in BackgroundGenerator(iterate_minibatches(num_examples,BATCH_SIZE, X_train, y_train)):\n",
    "            batch_x = np.array([x for x in b_x]) #np.fromiter(b_x,float)\n",
    "            batch_y = np.array([y for y in b_y]) #np.fromiter(b_y,float)\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: .8})\n",
    "        #for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            #end = offset + BATCH_SIZE\n",
    "            #batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            #sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy.accuracy()))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, 'lenetTS')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.954\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy.accuracy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e7643ad486c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtest_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gtsrb/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "def display_images(images, cmap=None, num_rows=3):\n",
    "    \"\"\"\n",
    "        Displays an image array a grid with number of rows=num_rows\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "    num_cols = int(num_images/num_rows)\n",
    "    fig, axes = plt.subplots(num_rows, num_cols)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Only plot the valid images\n",
    "        if i < num_images:\n",
    "            img = images[i]\n",
    "\n",
    "            # Plot image.\n",
    "            if(cmap):\n",
    "                ax.imshow(img, cmap=cmap)\n",
    "            else:\n",
    "                ax.imshow(img)\n",
    "\n",
    "def read_images(fileNames, display=True):\n",
    "    images = []\n",
    "    for file in fileNames:\n",
    "        image = cv2.imread(file)\n",
    "        image = image.astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images.append(image)\n",
    "    return images    \n",
    "        \n",
    "    \n",
    "\n",
    "test_files = [file for file in glob.glob(\"gtsrb/*\")]\n",
    "\n",
    "\n",
    "images = read_images(test_files)\n",
    "display_images(images, num_rows=4)\n",
    "\n",
    "x_collected = images\n",
    "y_collected_true = np.array([26, 11, 40, 12, 18, 11, 23, 14])\n",
    "y_collected_true_one_hot = encode_labels(y_collected_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
